\chapter{Global prediction}
\label{sec:global}

\section{Introduction}
\label{sec:global_introduction}
In this Chapter, an automatic approach to detect Atrial Fibrillation is analyzed and an attempt to improve it is made through Machine Learning techniques. The state of art it's based on different public datasets offered by PhysioNet \cite{physionet}, among which MIT-BIH Atrial Fibrillation Database \cite{afdb} and Long Term AF Database \cite{ltafdb} are used.
The method it's based on ECG, whose explanation it has been given in Section \ref{sec:ecg_diagnose}. The reason that lies behind the use of ECG, is its intrinsic simplicity, that cannot be found in methods like blood tests, chest x-ray, etc.

\subsection{Description of MIT-BIH}
The database includes $25$ long-term ECG recordings of patients with atrial fibrillation, which is mostly paroxysmal. Each record is $10$ hours in duration and contains two ECG signals sampled at 250 samples per second with $12-bit$ resolution over a range of $\pm 10$ millivolts. The signals files \verb|.dat| are available only on 23 records. But all of the records have \verb|.atr| and \verb|.qrs| annotations files. The former contains information about the kind of rhythm: atrial fibrillation, atrial flutter, junctional rhythm or other rhythms. The latter contains unaudited beat prepared using an automated detector and have not been corrected manually. In some cases, manually corrected beat annotations files \verb|.qrsc| are present.

\section{State of the art algorithm}
Most of the algorithms work on the processing of the ECGs components (P wave, QRS complex, ...) and the poorly coordinate atrial activation (AA) of heart and rapid cardiac beating. Although these pieces of information can lead to the identification of Atrial Fibrillation, noise must be taken into consideration. Especially with P waves which in general is of very low-intensity magnitude. Whereas the approaches based on the RR interval (R wave peak to R wave peak) irregularity, nonetheless the component is a more prominent feature of ECG and thus less subject to noise, tend to be quite complicated and not so efficient to make them suitable for real-time applications \cite[p. 2]{zhou2015}. For this reason, a real-time and low-complexity but robust method for the discrimination of AF episodes is taken as reference. The algorithm is composed of three steps and is based on the heart rate (HR).

\subsection{Heart rate sequence}
Let $hr_n$ be the heartbeat rate sequence obtained from,
\begin{equation}
hr_n = 60\, \text{s} \cdot \frac{250}{R_n - R_{n-1}}
\end{equation}
where $60$ are the seconds, $R_n$ is the sequence that denotes the $R$ peak in the $QRS$ complex and $250$ is the number of samples per second. From an implementation point of view, here one bpm is lost. The \verb|.qrs| files contain a registration where the first sample does not represent the $R$ peak, therefore all the part before the first $R$ peak cannot be used to compute the first bpm. An example of the sequence $hr_n$ can be found in \reffigtext{zhou_img}{(a)}. The following implementation of the function is done in Python 3.7
\inputpython{code/bpm.py}{1}{9}

\subsection{Symbolic dynamics of $hr_n$ sequence}
Let $sy_n$ denote a symbolic dynamics that encodes the information of $hr_n$ to a series with fewer symbols, where the mapping function is given by \cite[p. 3]{zhou2015},
\begin{equation}
sy_n = \begin{cases}
63 & \text{if $hr_n \ge 315$} \\
\lfloor hr_n / 5 \rfloor & \text{otherwise}
\end{cases}
\end{equation}
where $\lfloor \cdot \rfloor$ is a floor operator. In this way the raw sequence $hr_n$ is transformed in a sequence $sy_n \in [0, 63]$, with 64 instantaneous states \reffigtext{zhou_img}{(b)}. Here below the implementation
\inputpython{code/sy.py}{1}{2}

\subsection{History sequence of $sy_n$}
A 3-symbols template can be applied to get a window of information that acts as a history \reffigtext{zhou_img}{(c)}, in this case on 3 successive symbols. Through a novel operator defined below \cite[p. 3]{zhou2015}, the word value can be calculated. 
\begin{equation}
wv_n = (sy_{n-2} \times 2^{12}) + (sy_{n-1} \times 2^{6}) + sy_n
\end{equation}
A major intrinsic property to be explained is the following,
\begin{equation}
|wv| \, = \, |sy| - 2 \, = \, |hr| - 2 \, = \, |qrs| - 3  
\end{equation}
where $wv$ denotes the set of words in a specific moment. The set $qrs$ is composed of all the QRSs complexes provided by the ECG, in this case in the \verb|.qrs| files. To sum up, a tiny bit of information is lost, precisely 1 $R$ peak to compute the heartbeat sequence $hr$ and 2 bpm to compute the word sequence $wv$. Here follows the implementation of the above
\inputpython{code/wv.py}{1}{8}

\subsection{Shannon entropy}
\label{sec:entropy}
A coarser version of Shannon entropy is employed to discriminate the AF arrhythmias \reffigtext{zhou_img}{(d)}. Without loss of generality, let $\mathbf{A} = (A|P)$ denote a dynamic system. The unique elements in this set can be defined as $A = \{a_1, \ldots, a_k\}$ with the interrelated probability set $P = \{p_1, \ldots, p_k\} (1 \le k \le N)$, where $N$ is the total number of elements and $k$ are the unique elements in space $\mathbf{A}$. Each element $a_i$ has the probability $p_i = N_i/N (0 < p_i \le 1, \sum_{i=1}^{k}p_i=1)$, where $N_i$ is the total number of the specific element $a_i$ in space $\mathbf{A}$. Hence the coarser version of Shannon entropy can be defined to quantitatively calculate the information size of $wv_n$,
\begin{equation}\label{eq:entropy}
\mathcal{H}^{\prime\prime} (\mathbf{A}) = -\frac{k}{N \, log_2 N} \sum_{i=1}^{k}p_i \, log_2 p_i
\end{equation}
The dynamic $\mathcal{A}$ is characterized by a bin size of $N=127$ consecutive word elements from $wv_{n-126}$ to $wv_n$. By defining the characteristic set $A$ and the corresponding probability set $P$, the entropy $\mathcal{H}^{\prime\prime} (\mathbf{A})$ can be calculated. A specific cardiac beat $hr_n$ is labelled as AF if the coarser entropy meets or exceeds a discrimination optimal threshold equal to $0.639$. The threshold was obtained through an investigation of various thresholds in the range $[0.0, 1.0]$ with an increment of $0.001$ from the receiver operating characteristic (ROC) on training databases. 
The computational challenges that are found in the Equation \ref{eq:entropy} can be overcome with a pre-calculated map of $-\frac{1}{log_2 N}p_i \, log_2 pi_i$ \cite[p. 4]{zhou2015}. Here follows an implementation where a constant value $1000000$ it's used to get decimal floating points as integers through a floor operator.

\inputpython{code/entropy.py}{1}{24}

\fig{img/zhou.png}{0.8}{zhou_img}{Application of the method to detect AF. (a) is the original sequence $hr_n$; (b) is the symbolic dynamic $sy_n$; (c) the word sequence $wn_n$; (d) the distribution of $\mathcal{H}^{\prime\prime} (\mathbf{A})$.} 

\subsection{Results and comparisons}
\subsubsection{Performance metrics}
The work under consideration measures the performances using sensitivity ($Se$), specificity ($Sp$), positive predictive value ($PPV$), and overall accuracy ($ACC$) \cite[p. 6]{zhou2015}.
\begin{equation}
\begin{aligned} S e &=\frac{T P}{T P+F N},\qquad P P V=\frac{T P}{T P+F P} \\ S p &=\frac{T N}{T N+F P}, \qquad A C C=\frac{T P+T N}{T P+T N+F P+F N} \end{aligned}
\end{equation}
where $TP$ stands for true positives, $TN$ true negatives, $FP$ false positives and $FN$ false negatives.

\subsubsection{Training phase}
The training phase was executed on LTAFDB which consists of 84 long-term ECG recordings (commonly 24 to 25 hours duration) of patients with paroxysmal or persistent/permanent AF. The number of cardiac beats is around 9 million of which 59.2\% are annotated as AF. The threshold, as already mentioned in the previous section \ref{sec:entropy}, is tested from $0.0$ to $1.0$  with an increment of $0.001$. ROC curve is defined based on the metrics $Se$ and $1-Sp$, where a point on the graph is formed by the couple $(Se$; $1-Sp)$ of a specific threshold. Thus the best-case scenario, namely perfect classification, is the couple $a=(Se=1; 1-Sp=0)$, hence the best performance can be found trivially using the Euclidean distance of a generic point from the point $a$. In \reffig{zhou_img}, the best performance point $b$ is found at threshold $0.639$, with distance $0.0576$ from the perfect classification, an area under the ROC curve that is $0.9845$ and the corresponding values of $Se$, $Sp$, $PPV$ and $ACC$ are $96.14\%$, $95.73\%$, $97.03\%$ and $95.97\%$ respectively \cite{zhou2015}. In other words, a slight improvement is made compared to RRI based method Zhou, et al\cite{zhou2014}.

\fig{img/roc_zhou.png}{0.9}{zhou_img}{Application} 

\subsubsection{Testing phase}
This phase uses the threshold $0.639$ across all testing databases: AFDB, MITDB\cite{mitdb} and NSRDB\cite{nsrdb} set. A complete overview of the results of the state of art method explained and others can be found in Table \ref{table:zhou_hr_rri}.
AFDB$^1$ is experimentation without the records \verb|00735| and \verb|03665| for which the \verb|.hea| files are not existent. While AFDB$^2$ excludes the record \verb|04936| and \verb|05091| because many incorrect manual AF annotations are contained \cite{lee2013}. 
To be sure about experimentation, edge cases are needed. Hence dataset like MITDB which contains many coexisting various types of complex arrhythmias and NSRDB without any AF annotation, are perfect for this purpose.

\begin{table}[h]
\begin{threeparttable}
\caption[Performance comparison of some state of the art methods.]{Classification performance of different methods based on three different testing databases \cite[p. 8]{zhou2015}.}
\label{table:zhou_hr_rri}
\scriptsize
  \begin{tabularx}{\linewidth}{c c c c X c c c c}
  \toprule
  \textbf{Method} & \textbf{Feature} & \textbf{Year} & \textbf{Database} & \textbf{Temp} & \multicolumn{4}{c}{\textbf{Results}} \\
  \cline{6-9}
  \\
  & & & & & $\mathbf{SE(\%)}$ & $\mathbf{SP(\%)}$ & $\mathbf{PPV(\%)}$ & $\mathbf{ACC(\%)}$\\
  \midrule  
  Zhou, et al\cite{zhou2015} & HR & 2015 & AFDB & & 97.37 & 98.44 & 97.89 & 97.99\\
  & & & AFDB$^1$ & & 97.31 & 98.28 & 97.89 & 97.84 \\
  & & & AFDB$^2$ & & 98.43 & 98.46 & 97.92 & 98.45 \\
  & & & MITDB & & 97.83 & 87.41 & 47.67 & 88.51 \\
  & & & NSRDB & & NA & 99.68 & NA & NA \\
  \hline
  Petrênas, et al\cite{petrenas2015} & RRI & 2015 & AFDB & & 97.12 & 98.28 & - & -\\
  & & & AFDB$^1$ & & 97.1 & 98.1 & - & - \\
  & & & AFDB$^2$ & & 98.0 & 98.2 & - & - \\
  & & & MITDB & & 97.8 & 86.4 & 47.67 & 88.51 \\
  & & & NSRDB & & NA & 98.6 & NA & NA \\
  \hline
  Zhou, et al\cite{zhou2014} & RRI & 2014 & AFDB & & 96.89 & 98.25 & 97.62 & 97.67\\
  & & & AFDB$^1$ & & 96.82 & 98.06 & 97.61 & 97.50 \\
  & & & AFDB$^2$ & & 97.83 & 98.19 & 97.56 & 98.04 \\
  & & & MITDB & & 97.33 & 90.78 & 55.29 & 91.46 \\
  & & & NSRDB & & NA & 98.28 & NA & NA \\
  \hline
  Lee, et al\cite{lee2013} & RRI & 2014 & AFDB$^2$ & & 98.22 & 97.68 & - & 97.91\\
  & & & MITDB & & 91.1 & 89.7 & - & - \\
  & & & NSRDB & & NA & 99.7 & NA & NA \\
  \bottomrule
\end{tabularx}
\begin{tablenotes}
 	\item $^1$ Records \verb|00735| and \verb|03665| excluded.
	\item $^2$ Records \verb|04936| and \verb|05091| excluded.
	\item ‘NA’ indicates not applicable because there is no beat with AF reference annotation in this database.
    \end{tablenotes}
\end{threeparttable}
\end{table}

Thus the method performs statistically better than the others \cite[p. 11]{zhou2015} with a very low computational complexity \cite[p. 14]{zhou2015}

\section{Using machine learning techniques}
The state of the art algorithm just described could be improved using machine learning (ML) techniques. To make a complete experimentation, multiple ML algorithms should be used in the investigation. But first a replication of Zhou, et al\cite{zhou2015} is needed.

\subsection{State of the art replication}
The replication was made with Python 3.7 and its most popular libraries for data science, such as Numpy and Pandas. Wfdb library to manipulate the different databases introduced in Section \ref{sec:global_introduction}.
The core part of the work has been shown in pieces in the previous sections that explain the steps of the algorithm.

\begin{table}[h]
\begin{center}
\begin{threeparttable}
\caption[State of the art algorithm replication performance.]{State of the art algorithm replication performance.}
\label{table:zhou_hr_and_unimol}
\scriptsize
  \begin{tabular}{c c c c c c}
  \toprule
  \textbf{Method} & \textbf{Database} & \multicolumn{4}{c}{\textbf{Results}} \\
  \cline{3-6}
  \\
  & & $\mathbf{SE(\%)}$ & $\mathbf{SP(\%)}$ & $\mathbf{PPV(\%)}$ & $\mathbf{ACC(\%)}$\\
  \midrule  
  Zhou, et al\cite{zhou2015} & AFDB & 97.37 & 98.44 & 97.89 & 97.99 \\
  \hline
  $A$ & AFDB & 96.03 & 97.49 & 96.59 & 96.87 \\
  & AFDB$^3$ & 96.04 & 97.50 & 96.60 & 96.88 \\
  \hline
  $B$ & AFDB & 95.99 & 97.50 & 96.60 & 96.86 \\
  & AFDB$^3$ & 96.00 & 97.50 & 96.62 & 96.86 \\
  \hline
  $C^4$ & AFDB & 96.03 & 97.53 & 96.64 & 96.89 \\
  & AFDB$^3$ & 96.04 & 97.53 & 96.66 & 96.90 \\
  \bottomrule
\end{tabular}
\begin{tablenotes}
 	\item $^3$ File \verb|.qrsc| (qrs complexes corrected manually) used when available.
 	\item $^4$ Hybrid heartbeat rate were introduced. 584 $hr$ not classified.
    \end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

In Table \ref{table:zhou_hr_and_unimol} the performance of the replication are reported. The predicted values were compared with an oracle. To define the matching oracle $oa_n$ of a specific record, a binary sequence $bs_k$ was used to keep track of the samples that are AF (bit 1) and non-AF (bit 0), between the peaks $R_i$ and $R_{i+1}$. The correct labels were obtained from the \verb|.atr| files. Then the percentage of AF bit in the interval $RR$ was counted,
\begin{equation}
 \text{AF\%} = \frac{\# \text{ of ones}}{RR \text{ length}}
\end{equation}
In order to be able to carry out as complete trial as possible, the oracle $oa_n$ was defined based on the percentage in three different ways:
\begin{itemize}
\item Method $A$: $oa_n$ is AF $\iff AF\% = 1$, else non-AF
\item Method $B$: $oa_n$ is AF $\iff AF\% > 0.5$, else non-AF
\item Method $C$: $oa_n$ is $AF\%$
\end{itemize}
In the method $C$ $584$ beats were not classified, because hybrids (not $1$ and not $0$). In Table \ref{table:replication_error} the number of AF and non-AF beats classified per method are showed. An artefact was introduced in the implementation of algorithm or in the definition of the oracle. Further investigation are needed, but the difference between the methods applied on the same database is negligible.
%\inputpython{code/oracle.py}{1}{29}
\begin{table}
\begin{center}
\begin{threeparttable}
\caption{Number of beats comparison between state of the art and replication.}
\label{table:replication_error}
\scriptsize
  \begin{tabular}{c c c c c c}
  \toprule
  \textbf{Method} & \textbf{Database} & \textbf{AF} & \textbf{NON-AF} & \textbf{TOTAL} & \textbf{Difference from SOA}$^*$ \\
  \midrule  
  Zhou, et al\cite{zhou2015} & AFDB & 519687$^{**}$ & 701887$^{**}$ & 1221574 & $0$ \\
  B & AFDB & 516515 & 704969 & 1221484 & $-90$ beats\\
  B & AFDB$^3$ & 518082 & 705013 & 1223095 & $+1521$ beats \\
  \bottomrule
\end{tabular}
\begin{tablenotes}
 	\item $^*$ Difference from state of the art method.
 	\item $^{**}$ \cite[p. 9]{zhou2015}.
    \end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

\section{Applying machine learning}
During this phase, machine learning algorithms were applied on enriched feature datasets using Weka 3.8. The method $B$  on the corrected AFDB is the base of the experiment. Furthermore to make comparison more immediate, Matthews correlation coefficient (MCC), a measure of the quality of binary classification, was introduced
\begin{equation}
\mathrm{MCC}=\frac{TP \times TN-FP \times FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}
\end{equation}
From here on, a bespoke entropy $be_n$ is introduced on each dataset and is defined as follows. Let $hr_n$ be the heartbeat rate sequence. \\
\textbf{1 step}. Filter the $hr_n$ sequence by labelling beat $1$ if is stable otherwise $0$ or $2$:\\
\begin{equation}
\mathrm{x_n} = \begin{cases}
	0, & \text{if } hr_n \le 50 \\
	1, & \text{if } 50 < hr_n < 120 \\
    2,              & \text{otherwise} 
\end{cases}
\end{equation}
\textbf{2 step}. Count number of stable beat ($1$) in a window of 10 elements $[x_{n-9}, x_{n}]$:\\
\begin{equation}
\mathrm{be_n} = \sum_{i=n-9}^{n} [x_i=1]
\end{equation}

\subsubsection{Explicit and encoded entropy dataset}
As already explained in \ref{sec:entropy}, $hr_n$ is labelled as AF if the coarser entropy meets or exceeds a discrimination optimal threshold equal to 0.639. This will be referred as encoded entropy, while the value itself of the Shannon entropy is the explicit entropy. 

\begin{table}[h]
\begin{center}
\begin{threeparttable}
\caption[State of the art algorithm replication performance.]{Machine learning algorithms applied on dataset with explicit and encoded entropy, compared with the replication of Zhou and Zhou, et al \cite{zhou2015} itself.}
\label{table:zhou_hr_and_unimol}
\scriptsize
  \begin{tabular}{c c c c c c c}
  \toprule
  \textbf{Dataset} & \textbf{Algorithm} & \multicolumn{5}{c}{\textbf{Results}} \\
  \cline{3-7}
  \\
  & & $\mathbf{SE(\%)}$ & $\mathbf{SP(\%)}$ & $\mathbf{PPV(\%)}$ & $\mathbf{ACC(\%)}$ & $\mathbf{MCC(\%)}$\\
  \midrule  
  Explicit & j48 & 96.01 & 97.19 & 96.21 & 96.69 & 93.22 \\
  & ibk & 94.95 & 96.09 & 94.74 & 95.60 & 91.01 \\
  & logistic & \textbf{96.93} & 96.58 & 95.47 & 96.73 & 93.33 \\
  & bayesnet & 96.05 & 96.72 & 95.61 & 96.43 & 92.71 \\
  & adaboostm1 & 95.57 & 97.34 & 96.39 & 96.59 & 93.01 \\
  & randomforest & 95.19 & 96.10 & 94.78 & 95.71 & 91.24 \\
  & reptree & 95.38 & 97.32 & 96.36 & 96.49 & 92.83 \\
  \hline
  Encoded & j48 &  96.48 &  97.27 &  96.33 &  96.93 &  \textbf{93.73} \\
  & ibk &  96.48 &  97.27 &  96.33 &  96.93 &  \textbf{93.73} \\
  & logistic &  96.03 &  97.04 &  96.01 &  96.61 &  93.06 \\
  & bayesnet &  96.07 &  97.45 &  96.56 &  96.86 &  93.59 \\
  & adaboostm1 &  96.03 &  97.53 &  96.66 &  96.89 &  \textbf{93.64} \\
  & randomforest & 96.48 &  97.27 &  96.33 &  96.93 &  \textbf{93.73} \\
  & reptree & 96.48 &  97.27 &  96.33 &  96.93 &  \textbf{93.73} \\
  \hline
  Both & j48 & 95.98 & 97.19 & 96.21 & 96.68 & 93.20 \\
  & logistic & 96.67 & 96.91 & 95.87 & 96.81 & 93.48 \\
  & bayesnet & 96.05 & 97.53 & 96.65 & 96.90 & \textbf{93.65} \\
  & adaboostm1 & 95.57 & 97.34 & 96.39 & 96.59 & 93.01 \\
  & randomforest & 95.15 & 96.05 & 94.71 & 95.66 & 91.14 \\
  & reptree & 95.32 & 97.28 & 96.31 & 96.45 & 92.73 \\
  \hline
  AFDB & \textbf{replication} & 96.01 & 97.51 & 96.62 & 96.87 & 93.59 \\
  \hline
  AFDB & \textbf{zhou} & 97.37 & 98.44 & 97.89 & 97.99 & NA \\
  \bottomrule
\end{tabular}
\begin{tablenotes}
 	\item ‘NA’ indicates not applicable because there the metric is not offered by the reference \cite{zhou2015}.
    \end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

The first experiment consisted in using a dataset with encoded/explicit entropy, custom entropy and of course the oracle as a label (Table \ref{table:zhou_hr_and_unimol}). In the case of the explicit dataset (explicit and custom entropy with the oracle), the overall performance is lower than in the case of the replication. But a remarkable result is obtained in the case of the \verb|logistic| algorithm in term of $SE$ around $0.92$. The $MCC$ though of the \verb|logistic| is quite low compared to the replica. All in all, expected result, since the threshold was not used to discriminate against the Shannon entropy. 

As for the encoded case, an increase in performance was hypothesised, which proved to be true. In fact, five out of seven algorithms are able to obtain an increase in $MCC$ ($+0.14$) compared to the replica. It is important to underline that \verb|j48|, \verb|ibk|, \verb|random forest| and \verb|reptree| have the same performance and \verb|logistic| compared to before is the algorithm with the worst performance.

Instead for the last case where both entropias were used, a performance to report is certainly that of the algorithm \verb|bayesnet| with an increment in terms of $MCC$ equal to $(+0.06)$. In any case, the experiment in question is placed in the middle between the two previous ones.

Experimentation with the encoded dataset can be considered successful. If we consider that it has been applied on a basis that does not reach Zhou, we can assume that applying it on the latter, we should achieve improvements.

\subsubsection{Fast Fourier Transform and AR Coefficients}
The explicit and encoded datasets of the previous experimentation were used as a base, on which signal analysis was done excluding records \verb|03665| and \verb|00735| (no \verb|.hea| file). Each interval $[R_i, R_{i+1}]$ was divided into two blocks. For every block Fast Fourier Transform was applied obtaining 16 values and 4 Autoregressive model's coefficients estimated through Yule–Walker equations. 

\begin{table}[h]
\begin{center}
\begin{threeparttable}
\caption[State of the art algorithm replication performance.]{Machine learning algorithms applied on dataset with Fast Fourier Transform and AR coefficients, compared with the replication of Zhou and Zhou, et al \cite{zhou2015} itself.}
\label{table:zhou_unimol_fft_ar}
\scriptsize
  \begin{tabular}{c c c c c c c}
  \toprule
  \textbf{Dataset} & \textbf{Algorithm} & \multicolumn{5}{c}{\textbf{Results}} \\
  \cline{3-7}
  \\
  & & $\mathbf{SE(\%)}$ & $\mathbf{SP(\%)}$ & $\mathbf{PPV(\%)}$ & $\mathbf{ACC(\%)}$ & $\mathbf{MCC(\%)}$\\
  \midrule  
  Explicit FFT & j48 & 84.38 & 95.48 & 93.88 & 90.47 & 80.93 \\ 
  & logistic & \textbf{97.00} & 95.73 & 94.92 & 96.30 & 92.56 \\
  & adaboostm1 & 95.49 & 97.06 & 96.39 & 96.35 & 92.63 \\
  & randomforest & 88.78 & 96.82 & 95.83 & 93.19 & 86.36 \\
  & reptree &  84.82 & 95.06 & 93.39 & 90.44 & 80.82 \\
  \hline
  Encoded FFT & j48 & 89.01 & 93.36 & 91.68 & 91.40 & 82.61 \\
  & logistic & 96.27 & 96.61 & 95.89 & 96.46 & 92.85 \\
  & adaboostm1 & 95.97 & 97.26 & 96.65 & 96.68 & \textbf{93.29} \\
  & randomforest & 92.44 & 94.95 & 93.78 & 93.82 & 87.51 \\
  & reptree  & 90.85 & 93.40 & 91.89 & 92.25 & 84.34 \\
  \hline
  Explicit FFT with AR & j48 & 86.74 & 94.96 & 93.40 & 91.25 & 82.40 \\
  & logistic & 96.85 & 95.66 & 94.83 & 96.19 & 92.35 \\
  & adaboostm1 & 95.49 & 97.06 & 96.39 & 96.35 & 92.63 \\
  & randomforest & 92.30 & 97.18 & 96.42 & 94.98 & 89.89 \\
  & reptree & 89.86 & 95.23 & 93.94 & 92.81 & 85.49 \\
  \hline
  AFDB$^1$ & \textbf{replication} & 95.94 & 97.23 & 96.61 & 96.65 & 93.23 \\
  \hline
  AFDB & \textbf{zhou} & 97.37 & 98.44 & 97.89 & 97.99 & NA \\
  \bottomrule
\end{tabular}
\begin{tablenotes}
	\item $^1$ Records \verb|00735| and \verb|03665| excluded.
 	\item ‘NA’ indicates not applicable because there the metric is not offered by the reference \cite{zhou2015}.
    \end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}
Table \ref{table:zhou_unimol_fft_ar} shows results of the process. First clarification to do is that some algorithms were removed for performance issues.

In the case of explicit FFT dataset, 16 values were added to the previous dataset with explicit entropy and custom entropy. A several drop in overall performance, compared to the explicit dataset, occurred for most of the algorithms except for \verb|logistic| and \verb|adaboostm1| where it was just slight. The \verb|logistic| tended to find a great amount of true positives and in this case it reached its best performance in terms of $SE$ that was equal to $97.00\%$, facilitated by a dataset richer in information.

But as for encoded FFT dataset, the state of the art is outdated again but the increment in terms of $MCC$ is just a marginal $(+0.06)$ with the \verb|adaboostm1|. And if we consider the additional layer of complexity and an increment that is smaller than the previous one of $(+0.14)$ achieved by four algorithms, then we can say that is not successfull at all.

In the last case, coefficients of autoregressive model were used in addition to FFT over the explicit dataset. A general increment in the worst performing algorithms was achieved, but the \verb|logistic| decreased of $(-0.21)$ in terms of $MCC$ and the \verb|adaboostm1| remained completely unchanged. 

Thus because of the inconsistency in the statistical margins and the increase in complexity compared to the basic version of the datasets, this path can be considered as a failure.

\subsubsection{Transient values}
\label{table:zhou_hr_and_unimol}

\begin{table}[h]
\begin{center}
\begin{threeparttable}
\caption[State of the art algorithm replication performance.]{Machine learning algorithms applied on dataset with explicit and encoded entropy, compared with the replication of Zhou and Zhou, et al \cite{zhou2015} itself.}
\label{table:zhou_hr_and_unimol}
\scriptsize
  \begin{tabular}{c c c c c c c}
  \toprule
  \textbf{Dataset} & \textbf{Algorithm} & \multicolumn{5}{c}{\textbf{Results}} \\
  \cline{3-7}
  \\
  & & $\mathbf{SE(\%)}$ & $\mathbf{SP(\%)}$ & $\mathbf{PPV(\%)}$ & $\mathbf{ACC(\%)}$ & $\mathbf{MCC(\%)}$\\
  \midrule  
  Explicit & j48 & 96.08 & 97.19 & 96.23 & 96.72 & 93.29 \\
  & ibk & 94.99 & 96.14 & 94.83 & 95.65 & 91.11 \\
  & logistic & 96.97 & 96.60 & 95.50 & 96.76 & 93.40 \\
  & bayesnet & 96.12 & 96.73 & 95.64 & 96.47 & 92.79 \\
  & adaboostm1 & 95.65 & 97.33 & 96.39 & 96.61 & 93.07 \\
  & randomforest & 95.23 & 96.16 & 94.87 & 95.77 & 91.35 \\
  & reptree & 95.44 & 97.29 & 96.33 & 96.50 & 92.84 \\
  \hline
  Encoded & j48 & 96.56 & 97.26 & 96.34 & 96.96 & 93.79 \\
  & ibk & 96.56 & 97.26 & 96.34 & 96.96 & 93.79 \\
  & logistic & 96.11 & 97.53 & 96.66 & 96.92 & 93.71 \\
  & bayesnet & 96.13 & 97.45 & 96.57 & 96.89 & 93.64 \\
  & adaboostm1 & 96.11 & 97.53 & 96.66 & 96.92 & 93.71 \\
  & randomforest & 96.56 & 97.26 & 96.34 & 96.96 & 93.79 \\
  & reptree & 96.56 & 97.26 & 96.34 & 96.96 & 93.79 \\
  \hline
  AFDB & \textbf{replication} & 96.01 & 97.51 & 96.62 & 96.87 & 93.59 \\
  \hline
  AFDB & \textbf{zhou} & 97.37 & 98.44 & 97.89 & 97.99 & NA \\
  \bottomrule
\end{tabular}
\begin{tablenotes}
 	\item ‘NA’ indicates not applicable because there the metric is not offered by the reference \cite{zhou2015}.
    \end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

\section{Further developments}