\chapter{Global prediction}
\label{sec:global}

\section{Introduction}
\label{sec:global_introduction}
In this Chapter, an attempt to improve the state of the art through Machine Learning (ML) techniques is made. Specifically different algorithms are applied on datasets enriched by morphology data to detect atrial fibrillation.

\subsection{State of the art replication}
Before proceeding with the actual experiment, a replication of Zhou, et al\cite{zhou2015} is needed. Unfortunately the final results don't match between the original work and the replication.
The latter was made with Python 3.7 and the most popular libraries for data science, such as Numpy and Pandas. Wfdb library to manipulate the different databases introduced in Section \ref{sec:soa_introduction}.
The core part of the work has been shown in pieces in Chapter \ref{sec:state_of_the_art} that explains the steps of the algorithm.
\begin{table}[h]
\begin{center}
\begin{threeparttable}
\caption[State of the art algorithm replication performance.]{State of the art algorithm replication performance.}
\label{table:zhou_hr_and_unimol}
\scriptsize
  \begin{tabular}{c c c c c c}
  \toprule
  \textbf{Method} & \textbf{Database} & \multicolumn{4}{c}{\textbf{Results}} \\
  \cline{3-6}
  \\
  & & $\mathbf{SE(\%)}$ & $\mathbf{SP(\%)}$ & $\mathbf{PPV(\%)}$ & $\mathbf{ACC(\%)}$\\
  \midrule  
  Zhou, et al\cite{zhou2015} & AFDB & 97.37 & 98.44 & 97.89 & 97.99 \\
  \hline
  $A$ & AFDB & 96.03 & 97.49 & 96.59 & 96.87 \\
  & AFDB$^3$ & 96.04 & 97.50 & 96.60 & 96.88 \\
  \hline
  $B$ & AFDB & 95.99 & 97.50 & 96.60 & 96.86 \\
  & AFDB$^3$ & 96.00 & 97.50 & 96.62 & 96.86 \\
  \hline
  $C^4$ & AFDB & 96.03 & 97.53 & 96.64 & 96.89 \\
  & AFDB$^3$ & 96.04 & 97.53 & 96.66 & 96.90 \\
  \bottomrule
\end{tabular}
\begin{tablenotes}
 	\item $^3$ File \verb|.qrsc| (qrs complexes corrected manually) used when available.
 	\item $^4$ Hybrid heartbeats rate were introduced. 584 $hr$ not classified.
    \end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}
In Table \ref{table:zhou_hr_and_unimol} the performance of the replication is reported. The predicted values were compared with an oracle. To define the matching oracle $oa_n$ of a specific record, a binary sequence $bs_k$ was used to keep track of the samples that are AF (bit 1) and non-AF (bit 0), between the peaks $R_i$ and $R_{i+1}$. The correct labels were obtained from the \verb|.atr| files. Then the percentage of AF bit in the interval $RR$ was counted,
\begin{equation}
 \text{AF\%} = \frac{\# \text{ of ones}}{RR \text{ length}}
\end{equation}
In order to be able to carry out as complete trial as possible, the oracle $oa_n$ was defined based on the percentage in three different ways:
\begin{itemize}
\item Method $A$: $oa_n$ is AF $\iff AF\% = 1$, else non-AF
\item Method $B$: $oa_n$ is AF $\iff AF\% > 0.5$, else non-AF
\item Method $C$: $oa_n$ is $AF\%$
\end{itemize}
In method $C$ $584$ beats were not classified, because of hybrids (not $1$ and not $0$). In Table \ref{table:replication_error} the number of AF and non-AF beats classified per method are shown. An artefact was introduced in the implementation of the algorithm or the definition of the oracle. Further investigation is needed, but the difference between the methods applied to the same database is negligible.
%\inputpython{code/oracle.py}{1}{29}
\begin{table}
\begin{center}
\begin{threeparttable}
\caption{Number of beats comparison between state of the art and replication.}
\label{table:replication_error}
\scriptsize
  \begin{tabular}{c c c c c c}
  \toprule
  \textbf{Method} & \textbf{Database} & \textbf{AF} & \textbf{NON-AF} & \textbf{TOTAL} & \textbf{Difference from SOA}$^*$ \\
  \midrule  
  Zhou, et al\cite{zhou2015} & AFDB & 519687$^{**}$ & 701887$^{**}$ & 1221574 & $0$ \\
  B & AFDB & 516515 & 704969 & 1221484 & $-90$ beats\\
  B & AFDB$^3$ & 518082 & 705013 & 1223095 & $+1521$ beats \\
  \bottomrule
\end{tabular}
\begin{tablenotes}
 	\item $^*$ Difference from state of the art method.
 	\item $^{**}$ \cite[p. 9]{zhou2015}.
    \end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

\section{Applying machine learning}
During this phase, machine learning algorithms were applied on enriched feature datasets using Weka 3.8. Method $B$  on the corrected AFDB is the base of the experiment. Furthermore, to make the comparison more immediate, Matthews correlation coefficient (MCC), a measure of the quality of binary classification, was introduced
\begin{equation}
\mathrm{MCC}=\frac{TP \times TN-FP \times FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}
\end{equation}
From here on, bespoke entropy $be_n$ is introduced on each the dataset and is defined as follows. Let $hr_n$ be the heartbeat rate sequence. \\
\textbf{1 step}. Filter the $hr_n$ sequence by labelling beat $1$ if is stable otherwise $0$ or $2$:\\
\begin{equation}
\mathrm{x_n} = \begin{cases}
	0, & \text{if } hr_n \le 50 \\
	1, & \text{if } 50 < hr_n < 120 \\
    2,              & \text{otherwise} 
\end{cases}
\end{equation}
\textbf{2 step}. Count number of stable beats ($1$) in a window of 10 elements $[x_{n-9}, x_{n}]$:\\
\begin{equation}
\mathrm{be_n} = \sum_{i=n-9}^{n} [x_i=1]
\end{equation}

\subsubsection{Explicit and encoded entropy dataset}
As already explained in \ref{sec:entropy}, $hr_n$ is labelled as AF if the coarser entropy meets or exceeds a discrimination optimal threshold equal to 0.639. This will be referred as encoded entropy, while the value itself of the Shannon entropy is the explicit entropy. 

\begin{table}[h]
\begin{center}
\begin{threeparttable}
\caption{Machine learning algorithms applied on dataset with explicit and encoded entropy, compared with the replication of Zhou and Zhou, et al \cite{zhou2015} itself.}
\label{table:zhou_unimol_explicit_encoded}
\scriptsize
  \begin{tabular}{c c c c c c c}
  \toprule
  \textbf{Dataset} & \textbf{Algorithm} & \multicolumn{5}{c}{\textbf{Results}} \\
  \cline{3-7}
  \\
  & & $\mathbf{SE(\%)}$ & $\mathbf{SP(\%)}$ & $\mathbf{PPV(\%)}$ & $\mathbf{ACC(\%)}$ & $\mathbf{MCC(\%)}$\\
  \midrule  
  Explicit & j48 & 96.01 & 97.19 & 96.21 & 96.69 & 93.22 \\
  & ibk & 94.95 & 96.09 & 94.74 & 95.60 & 91.01 \\
  & logistic & \textbf{96.93} & 96.58 & 95.47 & 96.73 & 93.33 \\
  & bayesnet & 96.05 & 96.72 & 95.61 & 96.43 & 92.71 \\
  & adaboostm1 & 95.57 & 97.34 & 96.39 & 96.59 & 93.01 \\
  & randomforest & 95.19 & 96.10 & 94.78 & 95.71 & 91.24 \\
  & reptree & 95.38 & 97.32 & 96.36 & 96.49 & 92.83 \\
  \hline
  Encoded & j48 &  96.48 &  97.27 &  96.33 &  96.93 &  \textbf{93.73} \\
  & ibk &  96.48 &  97.27 &  96.33 &  96.93 &  \textbf{93.73} \\
  & logistic &  96.03 &  97.04 &  96.01 &  96.61 &  93.06 \\
  & bayesnet &  96.07 &  97.45 &  96.56 &  96.86 &  93.59 \\
  & adaboostm1 &  96.03 &  97.53 &  96.66 &  96.89 &  \textbf{93.64} \\
  & randomforest & 96.48 &  97.27 &  96.33 &  96.93 &  \textbf{93.73} \\
  & reptree & 96.48 &  97.27 &  96.33 &  96.93 &  \textbf{93.73} \\
  \hline
  Both & j48 & 95.98 & 97.19 & 96.21 & 96.68 & 93.20 \\
  & logistic & 96.67 & 96.91 & 95.87 & 96.81 & 93.48 \\
  & bayesnet & 96.05 & 97.53 & 96.65 & 96.90 & \textbf{93.65} \\
  & adaboostm1 & 95.57 & 97.34 & 96.39 & 96.59 & 93.01 \\
  & randomforest & 95.15 & 96.05 & 94.71 & 95.66 & 91.14 \\
  & reptree & 95.32 & 97.28 & 96.31 & 96.45 & 92.73 \\
  \hline
  AFDB & \textbf{replication} & 96.01 & 97.51 & 96.62 & 96.87 & 93.59 \\
  \bottomrule
\end{tabular}
\begin{tablenotes}
 	\item ‘NA’ indicates not applicable because there the metric is not offered by the reference \cite{zhou2015}.
    \end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

The first experiment consisted of using a dataset with encoded/explicit entropy, custom entropy and of course the oracle as a label (Table \ref{table:zhou_unimol_explicit_encoded}). In the case of the explicit dataset (explicit and custom entropy with the oracle), the overall performance was lower than in the case of the replication. But an outstanding increment was obtained in the case of the \verb|logistic| algorithm in term of $SE$ around $(+0.92)$. The $MCC$ though of the \verb|logistic| was quite low compared to the replica. All in all, the expected result, since the threshold was not used to discriminate against the Shannon entropy. 

As for the encoded case, an increase in performance was hypothesised, which proved to be true. Five out of seven algorithms were able to obtain an increase in $MCC$ ($+0.14$) compared to the replica. It is important to underline that \verb|j48|, \verb|ibk|, \verb|random forest| and \verb|reptree| had the same performances and \verb|logistic| in contrast to the first case, was the algorithm with the worst performance.

Instead for the last case where both entropies were used, a performance to report is certainly that of the algorithm \verb|bayesnet| with an increment in terms of $MCC$ equal to $(+0.06)$. In any case, the experiment in question was placed in the middle between the two previous ones.

Experimentation with the encoded dataset can be considered successful. If we consider that it has been applied on a basis that does not reach Zhou, we can assume that applying it on the latter, we should achieve improvements.

\subsubsection{Fast Fourier Transform and AR Coefficients}
The explicit and encoded datasets of the previous experimentation were used as a base, on which signal analysis was done excluding records \verb|03665| and \verb|00735| (no \verb|.hea| file). Each interval $[R_i, R_{i+1}]$ was divided into two blocks. For every block, Fast Fourier Transform was applied to obtain 16 values and 4 Autoregressive model's coefficients estimated through Yule–Walker equations. 

\begin{table}[h]
\begin{center}
\begin{threeparttable}
\caption{Machine learning algorithms applied on the dataset with Fast Fourier Transform and AR coefficients, compared with the replication of Zhou and Zhou, et al \cite{zhou2015} itself.}
\label{table:zhou_unimol_fft_ar}
\scriptsize
  \begin{tabular}{c c c c c c c}
  \toprule
  \textbf{Dataset} & \textbf{Algorithm} & \multicolumn{5}{c}{\textbf{Results}} \\
  \cline{3-7}
  \\
  & & $\mathbf{SE(\%)}$ & $\mathbf{SP(\%)}$ & $\mathbf{PPV(\%)}$ & $\mathbf{ACC(\%)}$ & $\mathbf{MCC(\%)}$\\
  \midrule  
  Explicit FFT & j48 & 84.38 & 95.48 & 93.88 & 90.47 & 80.93 \\ 
  & logistic & \textbf{97.00} & 95.73 & 94.92 & 96.30 & 92.56 \\
  & adaboostm1 & 95.49 & 97.06 & 96.39 & 96.35 & 92.63 \\
  & randomforest & 88.78 & 96.82 & 95.83 & 93.19 & 86.36 \\
  & reptree &  84.82 & 95.06 & 93.39 & 90.44 & 80.82 \\
  \hline
  Encoded FFT & j48 & 89.01 & 93.36 & 91.68 & 91.40 & 82.61 \\
  & logistic & 96.27 & 96.61 & 95.89 & 96.46 & 92.85 \\
  & adaboostm1 & 95.97 & 97.26 & 96.65 & 96.68 & \textbf{93.29} \\
  & randomforest & 92.44 & 94.95 & 93.78 & 93.82 & 87.51 \\
  & reptree  & 90.85 & 93.40 & 91.89 & 92.25 & 84.34 \\
  \hline
  Explicit FFT with AR & j48 & 86.74 & 94.96 & 93.40 & 91.25 & 82.40 \\
  & logistic & 96.85 & 95.66 & 94.83 & 96.19 & 92.35 \\
  & adaboostm1 & 95.49 & 97.06 & 96.39 & 96.35 & 92.63 \\
  & randomforest & 92.30 & 97.18 & 96.42 & 94.98 & 89.89 \\
  & reptree & 89.86 & 95.23 & 93.94 & 92.81 & 85.49 \\
  \hline
  AFDB$^1$ & \textbf{replication} & 95.94 & 97.23 & 96.61 & 96.65 & 93.23 \\
  \bottomrule
\end{tabular}
\begin{tablenotes}
	\item $^1$ Records \verb|00735| and \verb|03665| excluded.
 	\item ‘NA’ indicates not applicable because there the metric is not offered by the reference \cite{zhou2015}.
    \end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}
Table \ref{table:zhou_unimol_fft_ar} shows results of the process. First clarification to do is that some algorithms were removed for performance issues.

In the case of explicit FFT dataset, 16 values were added to the previous dataset with explicit entropy and custom entropy. A several drop in overall performance, compared to the explicit dataset, occurred for most of the algorithms except for \verb|logistic| and \verb|adaboostm1| where it was just slight. The \verb|logistic| tended to find a great number of true positives and in this case it reached its best performance in terms of $SE$ that was equal to $97.00\%$, facilitated by a dataset richer in information.

But as for encoded FFT dataset, the state of the art is outdated again but the increment in terms of $MCC$ was just a marginal $(+0.06)$ with the \verb|adaboostm1|. And if we consider the additional layer of complexity and an increment that is smaller than the previous one of $(+0.14)$ achieved by four algorithms, then we can say that is not successful at all.

In the last case, coefficients of the autoregressive model were used in addition to FFT over the explicit dataset. A general increment in the worst-performing algorithms was achieved, but the \verb|logistic| decreased of $(-0.21)$ in terms of $MCC$ and the \verb|adaboostm1| remained completely unchanged. 

Thus because of the inconsistency in the statistical margins and the increase in complexity compared to the basic version of the datasets, this path can be considered as a failure.

\subsubsection{Transient values}
\begin{table}[h]
\begin{center}
\begin{threeparttable}
\caption{Machine learning algorithms applied on the dataset with explicit and encoded entropy without the transient beats, compared with the replication of Zhou and Zhou, et al \cite[p. 7]{zhou2015} itself.}
\label{table:zhou_hr_and_unimol_126}
\scriptsize
  \begin{tabular}{c c c c c c c}
  \toprule
  \textbf{Dataset} & \textbf{Algorithm} & \multicolumn{5}{c}{\textbf{Results}} \\
  \cline{3-7}
  \\
  & & $\mathbf{SE(\%)}$ & $\mathbf{SP(\%)}$ & $\mathbf{PPV(\%)}$ & $\mathbf{ACC(\%)}$ & $\mathbf{MCC(\%)}$\\
  \midrule  
  Explicit & j48 & 96.08 & 97.19 & 96.23 & 96.72 & 93.29 \\
  & ibk & 94.99 & 96.14 & 94.83 & 95.65 & 91.11 \\
  & logistic & \textbf{96.97} & 96.60 & 95.50 & 96.76 & 93.40 \\
  & bayesnet & 96.12 & 96.73 & 95.64 & 96.47 & 92.79 \\
  & adaboostm1 & 95.65 & 97.33 & 96.39 & 96.61 & 93.07 \\
  & randomforest & 95.23 & 96.16 & 94.87 & 95.77 & 91.35 \\
  & reptree & 95.44 & 97.29 & 96.33 & 96.50 & 92.84 \\
  \hline
  Encoded & j48 & 96.56 & 97.26 & 96.34 & 96.96 & \textbf{93.79} \\
  & ibk & 96.56 & 97.26 & 96.34 & 96.96 & \textbf{93.79} \\
  & logistic & 96.11 & 97.53 & 96.66 & 96.92 & 93.71 \\
  & bayesnet & 96.13 & 97.45 & 96.57 & 96.89 & 93.64 \\
  & adaboostm1 & 96.11 & 97.53 & 96.66 & 96.92 & 93.71 \\
  & randomforest & 96.56 & 97.26 & 96.34 & 96.96 & \textbf{93.79} \\
  & reptree & 96.56 & 97.26 & 96.34 & 96.96 & \textbf{93.79} \\
  \hline
  AFDB & \textbf{replication} & 96.01 & 97.51 & 96.62 & 96.87 & 93.59 \\
  \hline
  AFDB$^1$ & \textbf{replication} & 96.11 & 97.53 & 96.66 & 96.92 & 93.71 \\
  \bottomrule
\end{tabular}
\begin{tablenotes}
	\item $^1$ dataset without the 126 transient beats.
 	\item ‘NA’ indicates not applicable because there the metric is not offered by the reference \cite{zhou2015}.
    \end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}
In \ref{sec:entropy} Shannon entropy has been defined on a bin with size $127$. Therefore, to avoid possible interpretations of the state of the art work, the first $126$ transient beats were removed, the $127nth$ value it's completely defined on the previous $126$ values, thus it was not removed. The dataset used as a base were the explicit and encoded dataset, because if there are improvements here consequently there are on the experimentations that use them as a starting point. Table \ref{table:zhou_hr_and_unimol_126} shows the results obtained.

In the case of the explicit dataset, logistic in terms of $SE$ obtained a remarkable increment of $(+0.86)$. But was not enough to compete with the replica results based on $AFDB^1$. If compared with the results in Table \ref{table:zhou_unimol_explicit_encoded}, an overall slight improvement in terms of $MCC$ was achieved and the distance between the replica and the best performance algorithm reduce from $(-0.26)$ to  $(-0.19)$.

As for the encoded dataset, the behaviour was quite similar to what happened in \ref{table:zhou_unimol_explicit_encoded}. In fact, \verb|j48|, \verb|ibk|, \verb|random forest| and \verb|reptree| had the same performances. But \verb|logistic| and \verb|adaboostm1| added nothing more to the replica. In terms of $MCC$ compared to the replica, the quartet of algorithms had an increment of around $(+0.08)$. All in all, the experiment without removing the 126 transient beats, obtained greater increments equal to $(+0.14)$ in terms of $MCC$.

\section{Improving specific aspects in AF detection}
After applying the above techniques, it is necessary to choose the experiment that had the best performance that is that on the encoded entropy dataset. The final conclusions on global prediction can be drawn by observing the Table \ref{table:overview_global_encoded} based on the mentioned dataset. The AFDB data set consists of 25 records, 12 of which were classified better than the state of the art algorithm. As regards the 3 records not reported in the table, there were neither improvements nor worsening compared to the basic version. Generally, however, there is a slight improvement in the number of correctly classified fibrillating beats.

\begin{table}[h]
\begin{center}
\begin{threeparttable}
\caption{Records with specific improvements of classification based on encoded entropy experiment.}
\label{table:overview_global_encoded}
\scriptsize
  \begin{tabular}{c c c c c c c}
  \toprule
  \textbf{Record} & \textbf{Algorithm} & \multicolumn{4}{c}{\textbf{Confusion matrix}} & \textbf{Surpasses}\\
  \cline{3-6}
  \\
  & & $\mathbf{TP}$ & $\mathbf{TN}$ & $\mathbf{FP}$ & $\mathbf{FN}$\\
  \midrule  
  05261 & j48 \& others & \textbf{771} & 43888 & 702 & \textbf{163} & $Se$\\
  & Zhou et, al & 654 & 44217 & 380 & 280 & -\\
  \hline
  07879$^*$ & j48 \& others & \textbf{39995} & 16447 & 102 & \textbf{40} & $Se, Acc, Mcc$ \\
  & logistic, bayesnet, adaboostm1 & \textbf{39945} & 16487 & 62 & \textbf{90}  & $Se$\\
  & Zhou et, al & 39943 & 16496 & 60 & 92 & -\\
  \hline
  06453$^*$ & All & \textbf{126} & \textbf{34288} & \textbf{94} & \textbf{319} & All metrics \\
  & Zhou et, al & 117 & 34272 & 117 & 328 & -\\
  \hline
  04043$^*$ & j48 \& others & \textbf{9170} & 44060 & 3211 & \textbf{5464} & $Se, Acc, Mcc$ \\
  & logistic, bayesnet, adaboostm1 & \textbf{8693} & 44211 & 3060 & \textbf{5941} & $Se, Acc, Mcc$ \\
  & Zhou et, al & 8544 & 44321 & 2957 & 6090 & - \\
  \hline
  05091 & j48 \& others & \textbf{4} & 36615 & 12 & \textbf{137} & $Se$ \\
  & Zhou et, al & 0 & 36634 & 0 & 141 & - \\
  \hline
  00735 & logistic, adaboostm1 & 275 & 39840 & \textbf{51} & 57 & $Sp, Ppv$ \\
  & Zhou et, al & 277 & 39846 & 52 & 55 & - \\
  \hline
  03665 & j48 \& others & \textbf{10995} & 41394 & 304 & \textbf{62} & $Se$ \\
  & Zhou et, al & 10985 & 41416 & 289 & 72 & - \\
  \hline
  08405$^*$ & All & \textbf{45019} & 13743 & 8 & \textbf{76} & $Se, Acc, Mcc$ \\
  & Zhou et, al & 45003 & 13758 & 0 & 92 & - \\
  \hline
  06995$^*$ & j48 \& others & \textbf{27028} & 25553 & 2110 & \textbf{488} & $Se$ \\
  & logistic, adaboostm1 & \textbf{26975} & \textbf{25645} & \textbf{2018} & \textbf{541} & $All$ \\
  & Zhou et, al & 26961 & 25640 & 2023 & 562 & - \\
  \hline
  07910 & j48 \& others & \textbf{6504} & 29589 & 230 & \textbf{266} & $Se$ \\
  & Zhou et, al & 6499 & 29722 & 104 & 271 & - \\
  \hline
  04908$^*$ & j48 \& others & \textbf{5526} & \textbf{55240} & \textbf{700} & \textbf{284} & $All$ \\
  & logistic & 5446 & \textbf{55279} & \textbf{661} & 364 & $Sp, Ppv, Acc, Mcc$ \\
  & Zhou et, al & 5491 & 55055 & 892 & 319 & - \\
  \hline
  08455$^*$ & j48 \& others & \textbf{44189} & 15238 & 40 & \textbf{75} & $Se, Acc, Mcc$ \\
  & Zhou et, al & 44103 & 15246 & 39 & 161 & - \\
  \hline
  05121 & j48 \& others & \textbf{32592} & \textbf{14976} & \textbf{1135} & 1168 & $Sp, Ppv$ \\
  & logistic, adaboostm1 & 32589 & \textbf{14986} & \textbf{1125} & 1171 & $Sp, Ppv$ \\
  & bayesnet & 32589 & \textbf{14981} & \textbf{1130} & 1171 & $Sp, Ppv$ \\
  & Zhou et, al & 32689 & 14923 & 1195 & 1071 & - \\
  \hline
  08378 & bayesnet, adaboostm1 & 10996 & 33886 & \textbf{142} & 481 & $Sp, Ppv$ \\
  & Zhou et, al & 11008 & 33891 & 144 & 469 & - \\
  \hline
  04015$^*$ & j48 \& others & \textbf{499} & 40714 & 2756 & \textbf{26} & $Se, Mcc$ \\
  & logistic, adaboostm1 & 483 & 40807 & \textbf{2663} & 42 & $Sp, Ppv, Mcc$ \\
  & bayesnet & \textbf{496} & 40771 & 2699 & \textbf{29} & $Se, Ppv, Mcc$ \\
  & Zhou et, al & 485 & 40812 & 2665 & 40 & - \\
  \hline
  06426 & logistic, adaboostm1 & 52014 & \textbf{799} & \textbf{1220} & 1112 & $Sp, Ppv$ \\
  & Zhou et, al & 52095 & 796 & 1223 & 1038 & - \\
  \hline
  04048$^*$ & j48 \& others & \textbf{632} & 38909 & 202 & \textbf{181} & $Se, Ppv, Acc, Mcc$ \\
  & bayesnet & \textbf{556} & 38930 & 181 & \textbf{257} & $Se, Acc, Mcc$ \\
  & Zhou et, al & 419 & 38982 & 136 & 394 & - \\
  \hline
  04936$^*$ & All & \textbf{33559} & 12086 & 1869 & \textbf{6122} & $Se, Acc, Mcc$ \\
  & Zhou et, al & 32662 & 12362 & 1600 & 7019 & - \\
  \hline
  04126$^*$ & j48 \& others & \textbf{3151} & 38664 & 893 & \textbf{142} & $Se, Acc, Mcc$ \\
  & logistic, bayesnet, adaboostm1 & 3017 & 38789 & \textbf{768} & 276 & $Sp$ \\
  & Zhou et, al & 3021 & 38795 & 769 & 272 & - \\
  \hline
  07162 & All & 39198 & 0 & 0 & \textbf{90} & $Se, Acc$ \\
  & Zhou et, al & 39198 & 0 & 0 & 97 & - \\
  \hline
  07859 & All & 61789 & 0 & 0 & \textbf{93} & $Se, Acc$ \\
  & Zhou et, al & 61789 & 0 & 0 & 100 & - \\
  \hline
  08219$^*$ & j48 \& others & \textbf{12953} & 42511 & 2578 & \textbf{1241} & $Se, Ppv, Acc, Mcc$ \\
  & logistic, bayesnet, adaboostm1 & 12643 & \textbf{42605} & \textbf{2484} & 1551 & $Sp, Ppv, Acc, Mcc$ \\
  & Zhou et, al & 12645 & 42555 & 2541 & 1549 & - \\
  \bottomrule
\end{tabular}
\begin{tablenotes}
	\item $^*$ record classified better than the state of the art.
	\item NOTE: \verb|j48|, \verb|ibk|, \verb|random forest| and \verb|reptree| had the same performances. First one is reported.
    \end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

\section{Further developments}
Further developments of the work could be more investigations on the gap of beats between the reply and what is reported on the reference paper. It is also necessary to understand what the artefact is between the performance of the replica and that of the paper.
Also, it is possible to use neural networks to obtain exhaustive experimentation of machine learning.